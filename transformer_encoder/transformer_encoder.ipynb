{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from tokenizer import Tokenizer\n",
    "from mlm_dataset.mlm_dataset_generator import MLMDatasetGenerator\n",
    "\n",
    "# # hyperparameters\n",
    "vocab_size = 24076\n",
    "model_dim = 512\n",
    "num_heads = 8\n",
    "ffn_dim = 2048\n",
    "max_pos = 512\n",
    "\n",
    "# test hyperparameters\n",
    "# vocab_size = 24076\n",
    "# model_dim = 70\n",
    "# num_heads = 2\n",
    "# ffn_dim = 2048\n",
    "# max_pos = 512\n",
    "\n",
    "# MLM dataset for training\n",
    "mlm_dataset_generator = MLMDatasetGenerator(max_pos=max_pos)\n",
    "\n",
    "mlm_dataset = mlm_dataset_generator.read_mlm_dataset_from_file()\n",
    "mlm_dataset_generator.read_raw_training_data_from_file()\n",
    "\n",
    "\n",
    "tokenizer = Tokenizer(max_pos=max_pos, vocab_size=vocab_size)\n",
    "\n",
    "# fit tokenizer on dataset\n",
    "tokenizer.fit_on_texts(mlm_dataset_generator.getVocubulary())\n",
    "\n",
    "# generate MLM dataset\n",
    "\n",
    "batch_size = 20\n",
    "sample_limit = 1000\n",
    "\n",
    "# to free memory\n",
    "del mlm_dataset_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([[13332, 21864, 21779, 15791,  5695,  2968,  2968,  2968,     0,\n",
      "            0,     0, 19560, 16895,  5899,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2]]), array([[1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]))\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.clean_truncate_tokenize_pad_atten('I am an example resume. Tae tae tae, meow mewo mewo. Cats dogs relationships.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22732\n"
     ]
    }
   ],
   "source": [
    "print(len(tokenizer.word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from base import Sequential\n",
    "from layers import WordEmbedding, PositionalEncoding, Dense, MultiHeadAttention, SelfAttention, LayerNormalization\n",
    "from activation import ReLu, Linear, Softmax\n",
    "from loss import CategoricalCrossEntropy\n",
    "from optimizers import Adam, GradientDescent\n",
    "\n",
    "model = Sequential([\n",
    "        WordEmbedding(vocab_size, model_dim),\n",
    "        PositionalEncoding(max_pos, model_dim),\n",
    "        MultiHeadAttention(num_heads, max_pos, model_dim),\n",
    "        # SelfAttention(max_pos, model_dim),\n",
    "        LayerNormalization(model_dim),\n",
    "        # Feed Forward Network\n",
    "        Dense([model_dim, ffn_dim], ReLu),\n",
    "        Dense([ffn_dim, model_dim], Linear),\n",
    "        LayerNormalization(model_dim),\n",
    "        # MLM Head\n",
    "        Dense([model_dim, vocab_size], Softmax)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data, validation_data, testing_data = mlm_dataset\n",
    "\n",
    "sample_limit = None\n",
    "\n",
    "training_tokens = training_data[:sample_limit * 2 if sample_limit else None:2]\n",
    "training_labels = training_data[1:sample_limit * 2 if sample_limit else None:2]\n",
    "\n",
    "# tokenization, padding, attention mask\n",
    "padded_tokenized_training_tokens, training_attention_mask = tokenizer.tokenize_pad_atten(tokens=training_tokens)\n",
    "\n",
    "# MLM training mask\n",
    "training_mlm_mask = tokenizer.generate_mlm_mask(training_attention_mask)\n",
    "\n",
    "# change padding tokens to 0\n",
    "training_attention_mask[training_attention_mask == -1] = 0\n",
    "padded_tokenized_training_tokens = np.array(padded_tokenized_training_tokens)\n",
    "padded_tokenized_training_tokens[padded_tokenized_training_tokens == -1] = tokenizer.get_pad_token_id()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_metric(Y, Y_hat):\n",
    "    Y, Y_hat = Y.T, Y_hat.T\n",
    "\n",
    "    rows, columns = np.where(Y == 1)\n",
    "    correct_predictions = sum(1 for actual, predicted in zip(columns, np.argmax(Y_hat[rows], axis=-1)) if actual == predicted)\n",
    "    return correct_predictions / len(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 10.088664214297545, Accuracy: 0.0, Elapsed Time: 19.770101070404053\n",
      "Loss: 10.089612972966897, Accuracy: 0.0, Elapsed Time: 16.030224561691284\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\M3OW\\School\\BSCS 4-3\\1st Sem\\Thesis Writing 2\\resume_checker\\transformer_encoder\\transformer_encoder.ipynb Cell 6\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/M3OW/School/BSCS%204-3/1st%20Sem/Thesis%20Writing%202/resume_checker/transformer_encoder/transformer_encoder.ipynb#X20sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     one_hot_labels[row, column] \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/M3OW/School/BSCS%204-3/1st%20Sem/Thesis%20Writing%202/resume_checker/transformer_encoder/transformer_encoder.ipynb#X20sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39m# np.add.at(one_hot_labels, (masked_token_indices, tokenized_labels), 1)\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/M3OW/School/BSCS%204-3/1st%20Sem/Thesis%20Writing%202/resume_checker/transformer_encoder/transformer_encoder.ipynb#X20sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m model\u001b[39m.\u001b[39;49mfit(X\u001b[39m=\u001b[39;49mpadded_tokenized, \n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/M3OW/School/BSCS%204-3/1st%20Sem/Thesis%20Writing%202/resume_checker/transformer_encoder/transformer_encoder.ipynb#X20sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m             attention_mask\u001b[39m=\u001b[39;49mattention_mask, \n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/M3OW/School/BSCS%204-3/1st%20Sem/Thesis%20Writing%202/resume_checker/transformer_encoder/transformer_encoder.ipynb#X20sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m             mlm_mask\u001b[39m=\u001b[39;49mmlm_mask, \n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/M3OW/School/BSCS%204-3/1st%20Sem/Thesis%20Writing%202/resume_checker/transformer_encoder/transformer_encoder.ipynb#X20sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m             Y\u001b[39m=\u001b[39;49mone_hot_labels, \n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/M3OW/School/BSCS%204-3/1st%20Sem/Thesis%20Writing%202/resume_checker/transformer_encoder/transformer_encoder.ipynb#X20sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m             loss_function\u001b[39m=\u001b[39;49mCategoricalCrossEntropy, \n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/M3OW/School/BSCS%204-3/1st%20Sem/Thesis%20Writing%202/resume_checker/transformer_encoder/transformer_encoder.ipynb#X20sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m             optimizer\u001b[39m=\u001b[39;49mGradientDescent(),\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/M3OW/School/BSCS%204-3/1st%20Sem/Thesis%20Writing%202/resume_checker/transformer_encoder/transformer_encoder.ipynb#X20sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m             accuracy_metric\u001b[39m=\u001b[39;49maccuracy_metric)\n",
      "File \u001b[1;32md:\\M3OW\\School\\BSCS 4-3\\1st Sem\\Thesis Writing 2\\resume_checker\\transformer_encoder\\base.py:74\u001b[0m, in \u001b[0;36mSequential.fit\u001b[1;34m(self, X, attention_mask, mlm_mask, Y, loss_function, optimizer, accuracy_metric)\u001b[0m\n\u001b[0;32m     72\u001b[0m dY \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers[layer_count \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mbackward(np\u001b[39m.\u001b[39marray([]), Y, mlm_masked_Y_hat, loss_function)\n\u001b[0;32m     73\u001b[0m \u001b[39mfor\u001b[39;00m j \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m((layer_count \u001b[39m-\u001b[39m \u001b[39m2\u001b[39m), \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[1;32m---> 74\u001b[0m     dY \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlayers[j]\u001b[39m.\u001b[39;49mbackward(dY)\n\u001b[0;32m     76\u001b[0m \u001b[39m# logger\u001b[39;00m\n\u001b[0;32m     77\u001b[0m accuracy_log \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m\n",
      "File \u001b[1;32md:\\M3OW\\School\\BSCS 4-3\\1st Sem\\Thesis Writing 2\\resume_checker\\transformer_encoder\\layers.py:371\u001b[0m, in \u001b[0;36mMultiHeadAttention.backward\u001b[1;34m(self, dY, Y, Y_hat, loss_function)\u001b[0m\n\u001b[0;32m    368\u001b[0m \u001b[39m# Backward pass through each attention head\u001b[39;00m\n\u001b[0;32m    369\u001b[0m dY_splits \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(np\u001b[39m.\u001b[39msplit(dY, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_heads, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m))\n\u001b[1;32m--> 371\u001b[0m gradients \u001b[39m=\u001b[39m [head\u001b[39m.\u001b[39mbackward(dY_split, Y, Y_hat, loss_function) \u001b[39mfor\u001b[39;00m head, dY_split \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattention_heads, dY_splits)]\n\u001b[0;32m    373\u001b[0m \u001b[39m# Sum the gradients from each attention head\u001b[39;00m\n\u001b[0;32m    374\u001b[0m total_gradient \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39msum(gradients, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n",
      "File \u001b[1;32md:\\M3OW\\School\\BSCS 4-3\\1st Sem\\Thesis Writing 2\\resume_checker\\transformer_encoder\\layers.py:371\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    368\u001b[0m \u001b[39m# Backward pass through each attention head\u001b[39;00m\n\u001b[0;32m    369\u001b[0m dY_splits \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(np\u001b[39m.\u001b[39msplit(dY, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_heads, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m))\n\u001b[1;32m--> 371\u001b[0m gradients \u001b[39m=\u001b[39m [head\u001b[39m.\u001b[39;49mbackward(dY_split, Y, Y_hat, loss_function) \u001b[39mfor\u001b[39;00m head, dY_split \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattention_heads, dY_splits)]\n\u001b[0;32m    373\u001b[0m \u001b[39m# Sum the gradients from each attention head\u001b[39;00m\n\u001b[0;32m    374\u001b[0m total_gradient \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39msum(gradients, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n",
      "File \u001b[1;32md:\\M3OW\\School\\BSCS 4-3\\1st Sem\\Thesis Writing 2\\resume_checker\\transformer_encoder\\layers.py:297\u001b[0m, in \u001b[0;36mSelfAttention.backward\u001b[1;34m(self, dY, Y, Y_hat, loss_function)\u001b[0m\n\u001b[0;32m    295\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbackward\u001b[39m(\u001b[39mself\u001b[39m, dY, Y\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, Y_hat\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, loss_function\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    296\u001b[0m     dA, dV \u001b[39m=\u001b[39m MatMul\u001b[39m.\u001b[39mbackward(dY\u001b[39m.\u001b[39mT, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattention_score, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalue_val)\n\u001b[1;32m--> 297\u001b[0m     dA \u001b[39m=\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m/\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdimension \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m \u001b[39m0.5\u001b[39m) \u001b[39m*\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__attention_derivative(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention_score, dA)\n\u001b[0;32m    298\u001b[0m     dQ, dK \u001b[39m=\u001b[39m MatMul\u001b[39m.\u001b[39mbackward(dA, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mquery_val, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkey_val\u001b[39m.\u001b[39mT)\n\u001b[0;32m    300\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mquery\u001b[39m.\u001b[39mbackward(dQ\u001b[39m.\u001b[39mT) \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkey\u001b[39m.\u001b[39mbackward(dK) \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalue\u001b[39m.\u001b[39mbackward(dV\u001b[39m.\u001b[39mT)\n",
      "File \u001b[1;32md:\\M3OW\\School\\BSCS 4-3\\1st Sem\\Thesis Writing 2\\resume_checker\\transformer_encoder\\layers.py:269\u001b[0m, in \u001b[0;36mSelfAttention.__attention_derivative\u001b[1;34m(attention, dY)\u001b[0m\n\u001b[0;32m    267\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(attention)):\n\u001b[0;32m    268\u001b[0m     dS \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mreshape(attention[i], (\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m))\n\u001b[1;32m--> 269\u001b[0m     dS \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mdiagflat(dS) \u001b[39m-\u001b[39;49m np\u001b[39m.\u001b[39;49mdot(dS, dS\u001b[39m.\u001b[39;49mT)\n\u001b[0;32m    270\u001b[0m     dY_hat\u001b[39m.\u001b[39mappend(dY[i]\u001b[39m.\u001b[39mdot(dS))\n\u001b[0;32m    272\u001b[0m \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39marray(dY_hat)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tokenized_training_labels = tokenizer.tokenize(training_labels)\n",
    "\n",
    "for _ in range(20):\n",
    "    for padded_tokenized, attention_mask, mlm_mask, labels in zip(padded_tokenized_training_tokens, training_attention_mask, training_mlm_mask, tokenized_training_labels):\n",
    "        \"\"\"\n",
    "            Creation of labels. Labels should be created per input to reduce memory usage.\n",
    "            Labels has a shape of (vocab_size, d_model) which is a very large data.\n",
    "        \"\"\"\n",
    "        one_hot_labels = np.zeros((max_pos, vocab_size), dtype=np.float64)\n",
    "        masked_token_indices = np.where(mlm_mask.flatten() == 1)[0].tolist()\n",
    "        for row, column in zip(masked_token_indices, labels):\n",
    "            one_hot_labels[row, column] = 1\n",
    "        # np.add.at(one_hot_labels, (masked_token_indices, tokenized_labels), 1)\n",
    "\n",
    "        model.fit(X=padded_tokenized, \n",
    "                    attention_mask=attention_mask, \n",
    "                    mlm_mask=mlm_mask, \n",
    "                    Y=one_hot_labels, \n",
    "                    loss_function=CategoricalCrossEntropy, \n",
    "                    optimizer=GradientDescent(),\n",
    "                    accuracy_metric=accuracy_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # convergence test\n",
    "# padded_tokenized, attention_mask, mlm_mask = list(zip(padded_tokenized_training_tokens, training_attention_mask, training_mlm_mask))[0]\n",
    "# tokenized_training_labels = tokenizer.tokenize(training_labels)[0]\n",
    "\n",
    "# for _ in range(20):\n",
    "#     \"\"\"\n",
    "#         Creation of labels. Labels should be created per input to reduce memory usage.\n",
    "#         Labels has a shape of (vocab_size, d_model) which is a very large data.\n",
    "#     \"\"\"\n",
    "#     one_hot_labels = np.zeros((max_pos, vocab_size), dtype=np.float64)\n",
    "#     masked_token_indices = np.where(mlm_mask.flatten() == 1)[0].tolist()\n",
    "#     for row, column in zip(masked_token_indices, tokenized_training_labels):\n",
    "#         one_hot_labels[row, column] = 1\n",
    "#     # np.add.at(one_hot_labels, (masked_token_indices, tokenized_labels), 1)\n",
    "\n",
    "#     model.fit(X=padded_tokenized, \n",
    "#                 attention_mask=attention_mask, \n",
    "#                 mlm_mask=mlm_mask, \n",
    "#                 Y=one_hot_labels, \n",
    "#                 loss_function=CategoricalCrossEntropy, \n",
    "#                 optimizer=GradientDescent(),\n",
    "#                 accuracy_metric=accuracy_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-4.27097538e-03  1.48845310e-03  6.13420199e-04 ... -2.24484553e-03\n",
      "  -4.61373138e-04 -3.72050150e-04]\n",
      " [ 4.45254348e-03  6.64050752e-03  3.39324109e-03 ...  5.36004780e-03\n",
      "   4.40420823e-03  2.55265890e-03]\n",
      " [ 1.11647376e-03  7.66161637e-04  2.85109310e-03 ...  1.59098767e-03\n",
      "   1.67978736e-03 -7.33809141e-05]\n",
      " ...\n",
      " [ 4.71714501e-04  3.59302877e-04  1.88894696e-03 ... -1.34735135e-03\n",
      "  -1.25241590e-03 -1.32853576e-03]\n",
      " [ 3.61046064e-03  2.04226710e-03  6.54083094e-04 ...  3.63204441e-03\n",
      "   4.01618613e-03  4.09725290e-03]\n",
      " [ 6.24274773e-04 -8.59859300e-04  2.10151943e-03 ... -8.59635046e-04\n",
      "  -3.59025337e-04 -2.00460710e-04]]\n",
      "(512, 512)\n"
     ]
    }
   ],
   "source": [
    "# # model beheading test\n",
    "# model.remove_mlm_head()\n",
    "\n",
    "# # convergence test\n",
    "# padded_tokenized, attention_mask, mlm_mask = list(zip(padded_tokenized_training_tokens, training_attention_mask, training_mlm_mask))[0]\n",
    "\n",
    "# embedding = model.predict(X=padded_tokenized, attention_mask=attention_mask)\n",
    "\n",
    "# print(embedding) # shape of (d_model, seq_length)\n",
    "# print(embedding.shape) # (512, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(model.get_trainable_variables())\n",
    "# model.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_model()\n",
    "# print(model.get_trainable_variables())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(model.get_trainable_variables())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
